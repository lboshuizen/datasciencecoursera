---
title: "Prediction Assignment Writeup"
output: html_notebook
author: L. Boshuizen
---

## Summary

Human Activity Recognition - HAR - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research. This HAR research has traditionally focused on discriminating between different activities, i.e. to predict "which" activity was performed at a specific point in time. The Weight Lifting Exercises dataset used in this study was collected to investigate "how (well)" an activity was performed by the wearer. The data is collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

The goal of this project is to predict the manner in which the participant in the study did the exercise. This is the "classe" variable in the training set.

3 Models are trained on the training set and combined into an ensemble:

1. Random Forest
2. Gradient Boost
3. Linear discriminant analysis

Random Forest and Gradient Boost deliver an accuracy of > .998. The ensemble results in the same performance as RF.

```{r message=F}
library(caret)
library(dplyr)
library(reshape2)
library(ggplot2)
library(readr)
library(randomForest)

set.seed(42)
```

## Exploration

### Data transformation

I took a different approach by investigating on wich variables a prediction should be made:

As it makes no sense to predict on vars that are NA, drop all vars from the test-set that are NA.
To be able to create a reliable model that can predict on the test-set, ONLY the variables that _remained_ in the _testset_ were _retained_ in the _trainingset_. __All others are dropped.__


```{r message=F}
df_train <- read_csv('./data/pml-training.csv')
df_test <- read_csv('./data/pml-testing.csv')

df_train <- df_train[,-1]
df_test <- df_test[,-1]

retain <- names(df_test)[colSums(!is.na(df_test)) > 0]
df_test <- df_test[, retain]
train_retain <- names(df_train) %in% retain
classe <- factor(df_train$classe)
df_train <- df_train[, train_retain]
df_train <- cbind(classe,df_train)

df_train$cvtd_timestamp <- NULL
df_train$new_window <- factor(df_train$new_window)

str(df_train)
```

### Correlation

From the correlation matrix below, it seems that there is almost no corrrelation in between. No need to remove more variables.

```{r}
nums <- sapply(df_train, is.numeric)
num_df <- df_train[nums]
corMat <- cor(num_df)
melted_cormat <- melt(corMat)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
    geom_tile() + theme(axis.text.x = element_text(angle = 90, hjust = 1))+labs(x = "", y="", title="Correlation of Numeric Variables")
```
## Modeling

3 Models are trained on 80% of the training set and each evaluated on the remaining 20% of the training set.

1. Random Forest
2. Grandient Boost
3. Linear discriminant analysis

Those 3 are then combined into a 4th ensemble model.

```{r}
ix_train <- createDataPartition(df_train$classe, p=.8, list=F)
train_set <- df_train[ix_train,]
test_set <- df_train[-ix_train,]
```

### Random forest (RF)

```{r}
m_rf <- randomForest(classe ~ ., data = train_set, importance = T)
p_rf <- predict(m_rf, test_set)
cfm_rf <- confusionMatrix(p_rf, test_set$classe)
cfm_rf
```

### Gradient Boosting (GBM)

```{r message=F}
m_bt <- train(classe ~ ., data = train_set,method="gbm", verbose=F)
p_gbm <- predict(m_bt, newdata = test_set)
cfm_gbm <- confusionMatrix(p_gbm,test_set$classe)
cfm_gbm
```

### Linear discriminant analysis (LDA)

```{r message=F, warning=FALSE}
m_lda <- train(classe ~ ., data = train_set,method="lda")
p_lda <- predict(m_lda, newdata = test_set)
cfm_lda <- confusionMatrix(p_lda,test_set$classe)
cfm_lda
```

### Ensemble

Random Forrest and Gradient Boost both have an accuracy of $\gt .998$. <br />
Combining the models into an ensemble to squeeze out some more accuracy

```{r}
df_c <- data.frame(rf=p_rf,gbm=p_gbm,lda=p_lda,classe=test_set$classe)
m_c <- train(classe ~ ., data=df_c, method="rf")
p_c <- predict(m_c, newdata = df_c)
cfm_ens <- confusionMatrix(p_c,test_set$classe)
cfm_ens
```

## Results

A table with the collected results:

| Model | Accuracy | 95% Conf. Interval | |
|-------|----------|----------|-|
| Random forest | `r cfm_rf$overall[1]` | `r cfm_rf$overall[3]` | `r cfm_rf$overall[4]` |
| GBM | `r cfm_gbm$overall[1]` | `r cfm_gbm$overall[3]` | `r cfm_gbm$overall[4]` |
| LDA | `r cfm_lda$overall[1]` | `r cfm_lda$overall[3]` | `r cfm_lda$overall[4]` |
| Ensemble | `r cfm_ens$overall[1]` | `r cfm_ens$overall[3]` | `r cfm_ens$overall[4]` |

Random forest & Gradient Boost already have an high accuracy, LDA lagging behind. 

Combining the models into an ensemble results in a model the same performance as RF.
